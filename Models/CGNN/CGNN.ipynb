{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the root directory to the location of Main and python files\n",
    "root_directory = 'C:/Users/U S VAITESSWAR/Desktop/For training/CGCNN'\n",
    "root_directory_2 = root_directory + '/cifs_dataset31'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import functools\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from pymatgen.core.structure import Structure\n",
    "\n",
    "class AtomInitializer(object):\n",
    "    \"\"\"\n",
    "    Base class for intializing the vector representation for atoms.\n",
    "    !!! Use one AtomInitializer per dataset !!!\n",
    "    \"\"\"\n",
    "    def __init__(self, atom_types):\n",
    "        self.atom_types = set(atom_types)\n",
    "        self._embedding = {}\n",
    "\n",
    "    def get_atom_fea(self, atom_type):\n",
    "        assert atom_type in self.atom_types\n",
    "        return self._embedding[atom_type]\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        self._embedding = state_dict\n",
    "        self.atom_types = set(self._embedding.keys())\n",
    "        self._decodedict = {idx: atom_type for atom_type, idx in\n",
    "                            self._embedding.items()}\n",
    "\n",
    "    def state_dict(self):\n",
    "        return self._embedding\n",
    "\n",
    "    def decode(self, idx):\n",
    "        if not hasattr(self, '_decodedict'):\n",
    "            self._decodedict = {idx: atom_type for atom_type, idx in\n",
    "                                self._embedding.items()}\n",
    "        return self._decodedict[idx]\n",
    "\n",
    "class AtomCustomJSONInitializer(AtomInitializer):\n",
    "    \"\"\"\n",
    "    Initialize atom feature vectors using a JSON file, which is a python\n",
    "    dictionary mapping from element number to a list representing the\n",
    "    feature vector of the element.\n",
    "    Parameters\n",
    "    ----------\n",
    "    elem_embedding_file: str\n",
    "        The path to the .json file\n",
    "    \"\"\"\n",
    "    def __init__(self, elem_embedding_file):\n",
    "        with open(elem_embedding_file) as f:\n",
    "            elem_embedding = json.load(f)\n",
    "        # elem_embedding is a dictonary containing feature vectors for the first 100 elements.\n",
    "        # The keys (atom number according to periodic table) in 'string' format and the values (atom features) are in list format.\n",
    "        # The keys are converted from string to integer format and restored as a dictionary.\n",
    "        elem_embedding = {int(key): value for key, value\n",
    "                          in elem_embedding.items()}\n",
    "        # Removing repeated keys in the dictionary\n",
    "        atom_types = set(elem_embedding.keys())\n",
    "        \n",
    "        super(AtomCustomJSONInitializer, self).__init__(atom_types)\n",
    "        # Converting the format of values (atomic features) from list to numpy array and \n",
    "        # restored into the dictionary again.\n",
    "        for key, value in elem_embedding.items():\n",
    "            self._embedding[key] = np.array(value, dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianDistance(object):\n",
    "    \"\"\"\n",
    "    Expands the distance by Gaussian basis.\n",
    "    Unit: angstrom\n",
    "    \"\"\"\n",
    "    def __init__(self, dmin, dmax, step, var=None):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        dmin: float\n",
    "          Minimum interatomic distance\n",
    "        dmax: float\n",
    "          Maximum interatomic distance\n",
    "        step: float\n",
    "          Step size for the Gaussian filter\n",
    "        \"\"\"\n",
    "        assert dmin < dmax\n",
    "        assert dmax - dmin > step\n",
    "        self.filter = np.arange(dmin, dmax+step, step)\n",
    "        if var is None:\n",
    "            var = step\n",
    "        self.var = var\n",
    "\n",
    "    def expand(self, distances):\n",
    "        \"\"\"\n",
    "        Apply Gaussian disntance filter to a numpy distance array\n",
    "        Parameters\n",
    "        ----------\n",
    "        distance: np.array shape n-d array\n",
    "          A distance matrix of any shape\n",
    "        Returns\n",
    "        -------\n",
    "        expanded_distance: shape (n+1)-d array\n",
    "          Expanded distance matrix with the last dimension of length\n",
    "          len(self.filter)\n",
    "        \"\"\"\n",
    "        return np.exp(-(distances[..., np.newaxis] - self.filter)**2 /\n",
    "                      self.var**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDs = pd.read_excel('Unique IDs.xlsx',header = None).values\n",
    "IDs = [IDs[i][0] for i in range(len(IDs))]\n",
    "atomic_features = AtomCustomJSONInitializer(root_directory_2 + '/atom_init.json')\n",
    "crystal_features = {}\n",
    "max_num_nbr = 12\n",
    "dmin = 0\n",
    "step = 0.2\n",
    "radius = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in IDs:\n",
    "    crystal = Structure.from_file(directory + '/' + i + '.cif')\n",
    "    atom_fea = np.vstack([atomic_features.get_atom_fea(crystal[i].specie.number) for i in range(len(crystal))])\n",
    "    all_nbrs = crystal.get_all_neighbors(radius, include_index=True)\n",
    "    all_nbrs = [sorted(nbrs, key=lambda x: x[1]) for nbrs in all_nbrs]\n",
    "    GDF = GaussianDistance(dmin = dmin, dmax = radius, step = step)\n",
    "    \n",
    "    nbr_fea_idx, nbr_fea = [], []\n",
    "    for nbr in all_nbrs:\n",
    "        if len(nbr) < max_num_nbr:\n",
    "            warnings.warn('{} not find enough neighbors to build graph. '\n",
    "                            'If it happens frequently, consider increase '\n",
    "                            'radius.'.format(i))\n",
    "\n",
    "            nbr_fea_idx.append(list(map(lambda x: x[2], nbr)) + [0] * (max_num_nbr - len(nbr)))              \n",
    "            nbr_fea.append(list(map(lambda x: x[1], nbr)) + [self.radius + 1.] * (max_num_nbr - len(nbr)))\n",
    "                                   \n",
    "        else:\n",
    "            nbr_fea_idx.append(list(map(lambda x: x[2],nbr[:max_num_nbr])))                               \n",
    "            nbr_fea.append(list(map(lambda x: x[1],nbr[:max_num_nbr])))\n",
    "                                        \n",
    "    nbr_fea_idx, nbr_fea = np.array(nbr_fea_idx), np.array(nbr_fea)\n",
    "    nbr_fea = GDF.expand(nbr_fea)\n",
    "    atom_fea = torch.Tensor(atom_fea)\n",
    "    nbr_fea = torch.Tensor(nbr_fea)\n",
    "    nbr_fea_idx = torch.LongTensor(nbr_fea_idx)\n",
    "    crystal_features[i] = [atom_fea,nbr_fea,nbr_fea_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# root_directory is the location which contains the python files (Data & Model)\n",
    "os.chdir(root_directory)\n",
    "\n",
    "import argparse\n",
    "import shutil\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "from random import sample\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn import metrics\n",
    "from torch.autograd import Variable\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "\n",
    "from Data import CIFData, collate_pool, get_train_val_test_loader\n",
    "from Model import CrystalGraphConvNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = True\n",
    "is_cuda = use_cuda and torch.cuda.is_available()\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalizer(object):\n",
    "    \"\"\"Normalize a Tensor and restore it later. \"\"\"\n",
    "\n",
    "    def __init__(self, tensor):\n",
    "        \"\"\"tensor is taken as a sample to calculate the mean and std\"\"\"\n",
    "        self.mean = torch.mean(tensor,dim = 0)\n",
    "        self.std = torch.std(tensor,dim = 0)\n",
    "\n",
    "    def norm(self, tensor):\n",
    "        return (tensor - self.mean) / self.std\n",
    "\n",
    "    def denorm(self, normed_tensor):\n",
    "        return normed_tensor * self.std + self.mean\n",
    "\n",
    "    def state_dict(self):\n",
    "        return {'mean': self.mean,\n",
    "                'std': self.std}\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        self.mean = state_dict['mean']\n",
    "        self.std = state_dict['std']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cust_loss(output, target_var):\n",
    "    \n",
    "    loss = 10**(torch.mean((target_var - output)**2))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, optimizer, epoch, normalizer_target, normalizer_crystal):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    mae_errors = AverageMeter()\n",
    "    \n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "    \n",
    "    loss_list = list()\n",
    "    \n",
    "    end = time.time()\n",
    "    for i, (input, target, _) in enumerate(train_loader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "        \n",
    "        # normalize crystal features \n",
    "        crystal_fea = normalizer_crystal.norm(input[4])\n",
    "        \n",
    "        if is_cuda:\n",
    "            input_var = (Variable(input[0].cuda(non_blocking=True)),\n",
    "                         Variable(input[1].cuda(non_blocking=True)),\n",
    "                         input[2].cuda(non_blocking=True),\n",
    "                         [crys_idx.cuda(non_blocking=True) for crys_idx in input[3]],\n",
    "                         Variable(crystal_fea).cuda(non_blocking=True))\n",
    "        else:\n",
    "            input_var = (Variable(input[0]),Variable(input[1]),input[2],input[3], Variable(crystal_fea))\n",
    "                           \n",
    "        # normalize target\n",
    "        target_normed = normalizer_target.norm(target)\n",
    "        \n",
    "        target_var = Variable(target_normed)\n",
    "\n",
    "        # compute output\n",
    "        output = model(*input_var)\n",
    "        \n",
    "        # Computing loss\n",
    "        loss = cust_loss(output, target_var)\n",
    "        loss_mean = loss.data.cpu()\n",
    "        loss_list.append(loss_mean)\n",
    "        \n",
    "        #print(output,target_var,loss.data.cpu())\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        mae_error = mae(normalizer_target.denorm(output.data.cpu()), target)\n",
    "        losses.update(loss.data.cpu(), target.size(0))\n",
    "        mae_errors.update(mae_error, target.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % 100 == 0: # print frequency = 100\n",
    "            \n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                      'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                      'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                      'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                      'MAE {mae_errors.val:.3f} ({mae_errors.avg:.3f})'.format(\n",
    "                    epoch, i, len(train_loader), batch_time=batch_time,\n",
    "                    data_time=data_time, loss=losses, mae_errors=mae_errors)\n",
    "                )\n",
    "                \n",
    "def validate(val_loader, model, normalizer_target, normalizer_crystal, test = False):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    mae_errors = AverageMeter()\n",
    "    \n",
    "    if test:\n",
    "        test_targets = []\n",
    "        test_preds = []\n",
    "        test_cif_ids = []\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "        \n",
    "    for i, (input, target, batch_cif_ids) in enumerate(val_loader):\n",
    "        with torch.no_grad():\n",
    "            crystal_fea = normalizer_crystal.norm(input[4])\n",
    "            \n",
    "            if is_cuda:\n",
    "                input_var = (Variable(input[0].cuda(non_blocking=True)),\n",
    "                             Variable(input[1].cuda(non_blocking=True)),\n",
    "                             input[2].cuda(non_blocking=True),\n",
    "                             [crys_idx.cuda(non_blocking=True) for crys_idx in input[3]],\n",
    "                             Variable(crystal_fea).cuda(non_blocking=True))\n",
    "            else:\n",
    "                input_var = (Variable(input[0]),\n",
    "                             Variable(input[1]),\n",
    "                             input[2],\n",
    "                             input[3],\n",
    "                             crystal_fea)\n",
    "\n",
    "        target_normed = normalizer_target.norm(target)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            target_var = Variable(target_normed)\n",
    "\n",
    "        # compute output\n",
    "        output = model(*input_var)\n",
    "        \n",
    "        # Computing loss\n",
    "        loss = cust_loss(output, target_var)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        mae_error = mae(normalizer_target.denorm(output.data.cpu()), target)\n",
    "        losses.update(loss.data.cpu().item(), target.size(0))\n",
    "        mae_errors.update(mae_error, target.size(0))\n",
    "        \n",
    "        if test:\n",
    "            test_pred = normalizer_target.denorm(output.data.cpu())\n",
    "            test_target = target\n",
    "            test_preds += test_pred.view(-1).tolist()\n",
    "            test_targets += test_target.view(-1).tolist()\n",
    "            test_cif_ids += batch_cif_ids\n",
    "            \n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "    \n",
    "        if i % 100 == 0:\n",
    "                print('Test: [{0}/{1}]\\t' # print frequency = 100\n",
    "                      'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                      'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                      'MAE {mae_errors.val:.3f} ({mae_errors.avg:.3f})'.format(\n",
    "                      i, len(val_loader), batch_time=batch_time, loss=losses,\n",
    "                      mae_errors=mae_errors))\n",
    "            \n",
    "    if test:\n",
    "        star_label = '**'\n",
    "        import csv\n",
    "        with open('test_results.csv', 'w') as f:\n",
    "            writer = csv.writer(f)\n",
    "            for cif_id, target, pred in zip(test_cif_ids, test_targets,\n",
    "                                            test_preds):\n",
    "                writer.writerow((cif_id, target, pred))\n",
    "    else:\n",
    "        star_label = '*'\n",
    "    \n",
    "    if test:\n",
    "        print(' {star} MAE {mae_errors.avg:.3f}'.format(star=star_label,mae_errors=mae_errors))                                             \n",
    "        return test_preds,test_targets\n",
    "    else:\n",
    "        return mae_errors.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae(prediction, target):\n",
    "    \"\"\"\n",
    "    Computes the mean absolute error between prediction and target\n",
    "    Parameters\n",
    "    ----------\n",
    "    prediction: torch.Tensor (N, 1)\n",
    "    target: torch.Tensor (N, 1)\n",
    "    \"\"\"\n",
    "    prediction = 10**prediction\n",
    "    target = 10**target\n",
    "    errors = ((target - prediction)/target)*100\n",
    "    MAE = torch.mean(torch.abs(errors))\n",
    "    \n",
    "    return MAE\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, 'model_best.pth.tar')\n",
    "\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch, k):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every k epochs\"\"\"\n",
    "    assert type(k) is int\n",
    "    lr = args.lr * (0.1 ** (epoch // k))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CIFData(crystal_features,root_directory_2)\n",
    "collate_fn = collate_pool\n",
    "\n",
    "train_loader, val_loader, test_loader = get_train_val_test_loader(\n",
    "    dataset = dataset,\n",
    "    collate_fn = collate_fn,\n",
    "    batch_size = 128,\n",
    "    train_ratio = 0.8,\n",
    "    num_workers = 0, # All workers\n",
    "    val_ratio = 0.1,\n",
    "    test_ratio = 0.1,\n",
    "    pin_memory = is_cuda,\n",
    "    train_size = None,\n",
    "    val_size = None,\n",
    "    test_size = None,\n",
    "    return_test = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling 500 data points at random from dataset\n",
    "sample_data_list = [dataset[i] for i in sample(range(len(dataset)), 500)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_input, sample_target, _ = collate_pool(sample_data_list)\n",
    "structures, _, _ = dataset[0] # Extracting only the first element of the data set\n",
    "normalizer_target = Normalizer(sample_target)\n",
    "normalizer_crystal = Normalizer(sample_input[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_atom_fea_len = structures[0].shape[-1] # Number of features in the atomic feature vector\n",
    "nbr_fea_len = structures[1].shape[-1] # Number of features in the neighbor feature vector\n",
    "crystal_fea_len = structures[3].shape[-1] # Number of additional crystal features\n",
    "\n",
    "model = CrystalGraphConvNet(orig_atom_fea_len, nbr_fea_len,\n",
    "                            atom_fea_len = 50, # First layer of linear transformation before convolution\n",
    "                            n_conv = 3, # Number of convolution layers\n",
    "                            h_fea_len = 100, # Number of units in first hidden layer of fully connected network\n",
    "                            n_h = 2, # Number of fully connected layers\n",
    "                            crystal_fea_len = crystal_fea_len,\n",
    "                            classification = False) # Regression\n",
    "\n",
    "if is_cuda:\n",
    "    model.cuda()\n",
    "    \n",
    "#criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), 0.0001, weight_decay = 0.1)\n",
    "scheduler = MultiStepLR(optimizer, milestones = [100],gamma = 0.1)\n",
    "\n",
    "start_epoch = 1\n",
    "end_epoch = 11 # 10 epochs\n",
    "\n",
    "for epoch in range(start_epoch, end_epoch):\n",
    "    \n",
    "    # train for one epoch\n",
    "    train(train_loader, model, optimizer, epoch, normalizer_target, normalizer_crystal)\n",
    "\n",
    "    # evaluate on validation set\n",
    "    mae_error = validate(val_loader, model, normalizer_target, normalizer_crystal)\n",
    "\n",
    "    if mae_error != mae_error:\n",
    "        print('Exit due to NaN')\n",
    "        sys.exit(1)\n",
    "\n",
    "    scheduler.step()\n",
    "    \n",
    "    best_mae_error = 1e10\n",
    "    \n",
    "    # remember the best mae_eror and save checkpoint\n",
    "    is_best = mae_error < best_mae_error\n",
    "    best_mae_error = min(mae_error, best_mae_error)\n",
    "\n",
    "    save_checkpoint({\n",
    "        'epoch': epoch + 1,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'best_mae_error': best_mae_error,\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'normalizer': normalizer_target.state_dict(),\n",
    "    }, is_best)\n",
    "\n",
    "# test best model\n",
    "print('---------Evaluate Model on Test Set---------------')\n",
    "best_checkpoint = torch.load('model_best.pth.tar')\n",
    "model.load_state_dict(best_checkpoint['state_dict'])\n",
    "test, target = validate(test_loader, model, normalizer_target, normalizer_crystal, test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xlsxwriter\n",
    "\n",
    "workbook = xlsxwriter.Workbook('Full dataset (CGCNN test results).xlsx')\n",
    "worksheet = workbook.add_worksheet()\n",
    "row = 0\n",
    "col = 0\n",
    "\n",
    "# Iterate over the data and write it out row by row\n",
    "for i in range(len(target)):\n",
    "    worksheet.write(row, col, target[i])\n",
    "    worksheet.write(row, col + 1, test[i])\n",
    "\n",
    "\n",
    "    row += 1\n",
    "\n",
    "workbook.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
